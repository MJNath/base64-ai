{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a416ec85",
   "metadata": {},
   "source": [
    "## üß† Qu‚Äôest-ce qu‚Äôun mod√®le de chat ?\n",
    "\n",
    "Les **mod√®les de chat** sont des interfaces qui permettent d'interagir avec des **grands mod√®les de langage (LLM)** via une s√©rie de messages, simulant ainsi une conversation humaine. LangChain fournit une interface coh√©rente pour travailler avec ces mod√®les, ind√©pendamment du fournisseur, tout en offrant des fonctionnalit√©s suppl√©mentaires pour le suivi, le d√©bogage et l'optimisation des performances des applications utilisant des LLM\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Fonctionnalit√©s principales\n",
    "\n",
    "- üîß **Appel d‚Äôoutils (Tool Calling)** : permet aux mod√®les d'ex√©cuter des fonctions personnalis√©es (APIs, outils externes, agents...).\n",
    "- üßæ **Sortie structur√©e** : g√©n√®re des r√©ponses JSON selon un sch√©ma d√©fini.\n",
    "- üñºÔ∏è **Multimodalit√©** : certains mod√®les traitent du texte, mais aussi des images ou du son.\n",
    "- üåä **Streaming** : renvoie les r√©ponses progressivement, token par token.\n",
    "- üß∫ **Batching** : traitement de plusieurs requ√™tes en parall√®le pour optimiser les appels.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Param√®tres standards des mod√®les\n",
    "\n",
    "| Param√®tre        | Description                                                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|\n",
    "| `model`          | Nom du mod√®le √† utiliser (ex. `gpt-4`, `claude-3-opus`).                    |\n",
    "| `temperature`    | Contr√¥le la cr√©ativit√© : `0` (r√©ponse fiable) ‚Üí `1` (r√©ponse imaginative).   |\n",
    "| `max_tokens`     | Nombre maximum de tokens g√©n√©r√©s.                                           |\n",
    "| `stop`           | Liste de s√©quences o√π la g√©n√©ration doit s‚Äôarr√™ter.                         |\n",
    "| `timeout`        | D√©lai maximal d‚Äôattente de r√©ponse (en secondes).                           |\n",
    "| `max_retries`    | Nombre de tentatives en cas d‚Äô√©chec de l‚Äôappel.                             |\n",
    "| `api_key`        | Cl√© API pour authentification.                                              |\n",
    "| `base_url`       | URL personnalis√©e du fournisseur si n√©cessaire.                             |\n",
    "| `rate_limiter`   | Contr√¥le du d√©bit pour √©viter les erreurs de quota.                         |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è M√©thodes disponibles\n",
    "\n",
    "- `invoke(messages)`  \n",
    "  Appel standard d‚Äôun mod√®le de chat avec une liste de messages.\n",
    "\n",
    "- `stream(messages)`  \n",
    "  Retourne la r√©ponse en temps r√©el, id√©ale pour le live chat.\n",
    "\n",
    "- `with_structured_output(schema)`  \n",
    "  Contraint la sortie √† respecter un format structur√© (ex : dictionnaire Python).\n",
    "\n",
    "- `bind_tools(tools)`  \n",
    "  Connecte des outils externes √† utiliser durant la conversation.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Fournisseurs compatibles\n",
    "\n",
    "LangChain supporte les principaux fournisseurs :\n",
    "- **OpenAI**\n",
    "- **Anthropic**\n",
    "- **Azure OpenAI**\n",
    "- **Ollama**\n",
    "- **Amazon Bedrock**\n",
    "- **Google Vertex AI**\n",
    "- **Hugging Face**\n",
    "- **Groq**, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d13930",
   "metadata": {},
   "source": [
    "## Mod√®le d'invocation des chatmodels \n",
    "Par convention : \n",
    "\n",
    "- Le nom du module lors de l'installation est ecrit suivant le mod√®le suivant : \"langchain-fournisseur\"\n",
    "- Le nom de du module lors de l'importation est ecrit suivant le mod√®le suivant : \"langchain_fournisseur\"\n",
    "- Le nom de la classe qui fait office d'interface d'appel du mod√®le est ecrit suivant le mod√®le suivant : \"ChatFournisseur\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bd721",
   "metadata": {},
   "source": [
    "### en fonction des fournisseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2f2c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPEL OPENAI :  Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "-------------\n",
      "APPEL GEMINI :  Bonjour ! Comment puis-je vous aider aujourd'hui ?\n",
      "-------------\n",
      "APPEL DEEPSEEK :  Bonjour ! üòä Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm_openai = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "call_openai = llm_openai.invoke(\"Bonjour\")\n",
    "\n",
    "print(\"APPEL OPENAI : \",call_openai.content)\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "call_gemini = llm_gemini.invoke(\"Bonjour\")\n",
    "print(\"-------------\")\n",
    "\n",
    "print(\"APPEL GEMINI : \", call_gemini.content)\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "\n",
    "llm_deepseek = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "call_deepseek = llm_deepseek.invoke(\"Bonjour\")\n",
    "print(\"APPEL DEEPSEEK : \",call_deepseek.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4031ee3",
   "metadata": {},
   "source": [
    "### en utilisant l'interface abstraite global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "def mixture_of_models(model : str, input: str):\n",
    "  if model.startswith(\"gpt\"):\n",
    "    llm_global = init_chat_model(model=model, model_provider=\"openai\")\n",
    "  elif model.startswith(\"deep\"):\n",
    "    llm_global = init_chat_model(model=model, model_provider=\"deepseek\")\n",
    "\n",
    "  return llm_global.invoke(input)\n",
    "\n",
    "\n",
    "call = mixture_of_models(\"gpt-4o-mini\", \"quelle la racine carr√© de 25\")\n",
    "print(type(call))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
