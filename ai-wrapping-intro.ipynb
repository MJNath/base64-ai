{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea520008",
   "metadata": {},
   "source": [
    "## APPELS A QUELQUES LLMS A PARTIR DES APIs OFFICIELLES \n",
    "\n",
    "\n",
    "⚠️ Problème\n",
    "\n",
    "Les LLMs hallicunent. \n",
    "\n",
    "Les LLMS sont Stateless, c'est à dire sans memoire.\n",
    "\n",
    "Les sorties des LLMs ne sont pas uniformes :\n",
    "\n",
    "- OpenAI Chat: response.choices[0].message.content\n",
    "\n",
    "- OpenAI Responses: response.output[0].content[0].text\n",
    "\n",
    "- Gemini: response.candidates[0].content.parts[0].text\n",
    "\n",
    "Chaque API a son propre format, ce qui complique :\n",
    "\n",
    "- l’extraction du texte,\n",
    "\n",
    "- la maintenance du code,\n",
    "\n",
    "- l’orchestration multi-fournisseur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4807a1a",
   "metadata": {},
   "source": [
    "### APPEL EN UTILISANT LA NOUVELLE METHODE DE OPENAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69895284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_681b90ca95fc819290943375653822e506daad0c66175474', created_at=1746637004.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_681b90d01e848192809c160c3ca15b1106daad0c66175474', content=[ResponseOutputText(annotations=[], text='Bonjour ! Je vais bien, merci. Et toi, comment vas-tu ?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=12, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=16, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI  # Importe la classe OpenAI depuis le SDK (à condition d'utiliser la version compatible)\n",
    "from dotenv import load_dotenv  # Importe la fonction permettant de charger les variables d’environnement depuis un fichier .env\n",
    "\n",
    "load_dotenv()  # Charge les variables d’environnement définies dans le fichier .env (ex : OPENAI_API_KEY)\n",
    "\n",
    "client = OpenAI()  # Initialise le client OpenAI (⚠️ Peut nécessiter un paramètre api_key selon la version du SDK)\n",
    "\n",
    "call = client.responses.create(  \n",
    "    input = \"Bonjour, comment vas tu\",  # Entrée utilisateur à envoyer au modèle\n",
    "    model = \"gpt-4.1-nano\"  # Nom du modèle à utiliser\n",
    ")\n",
    "\n",
    "print(call)  # Affiche la réponse brute reçue de l’API (si la requête fonctionne)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b4e83",
   "metadata": {},
   "source": [
    "### APPEL EN UTILISANT L'ANCIENNE METHODE DE OPENAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923e2839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BUcMbkhvXGQqdYCesT7Bk7k4H0HJX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Bonjour ! Je vais bien, merci. Comment puis-je vous aider aujourd'hui ?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746637009, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_f5bdcc3276', usage=CompletionUsage(completion_tokens=17, prompt_tokens=13, total_tokens=30, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI  # Importation du client OpenAI pour interagir avec l'API\n",
    "from dotenv import load_dotenv  # Permet de charger les variables d'environnement depuis un fichier .env\n",
    "\n",
    "load_dotenv()  # Chargement des variables d’environnement, dont la clé OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()  # Initialisation du client avec la clé API chargée\n",
    "\n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4o\",  \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Bonjour, comment vas-tu ?\"}  # Message envoyé par l'utilisateur\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)  # Affichage de la réponse générée par l’IA dans la console\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed2687",
   "metadata": {},
   "source": [
    "### APPEL EN UTILISANT L'API DE GEMINI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492a20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Je vais bien, merci de demander ! Comment allez-vous ?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.08828901393072945, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)] create_time=None response_id=None model_version='gemini-2.0-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=14, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=14)], prompt_token_count=3, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=3)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=17, traffic_type=None) automatic_function_calling_history=[] parsed=None\n"
     ]
    }
   ],
   "source": [
    "from google import genai  # Importe la bibliothèque officielle de Google pour interagir avec les modèles Gemini\n",
    "import os  # Permet d'accéder aux variables d’environnement\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")  # Récupère la clé API Gemini stockée dans les variables d’environnement\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_KEY)  # Initialise le client Gemini avec la clé API pour authentification\n",
    "\n",
    "response = client.models.generate_content( \n",
    "    model=\"gemini-2.0-flash\",  \n",
    "    contents=\"Comment vas tu\", \n",
    ")\n",
    "\n",
    "print(response)  # Affiche la réponse brute retournée par l'API (peut contenir texte, métadonnées, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a79cb1c",
   "metadata": {},
   "source": [
    "### Hallicination \n",
    "\n",
    "L’hallucination, dans le contexte des modèles de langage, désigne le phénomène où l’IA génère une réponse fausse, inventée ou non vérifiable, tout en la présentant comme crédible ou factuelle.\n",
    "\n",
    "- Reponse de l'IA: \n",
    "\n",
    "LangChain est une plateforme de blockchain basée sur Ethereum qui propose une solution de traduction automatique de haute qualité en exploitant la technologie de la blockchain. Elle vise à offrir des services de traduction rapides, précis et sécurisés en utilisant des algorithmes d'apprentissage automatique avancés. LangChain cherche à révolutionner le secteur de la traduction en tirant parti de la technologie blockchain pour améliorer l'efficacité et la qualité des traductions.\n",
    "\n",
    "- Reponse Correcte : \n",
    "\n",
    "LangChain est un framework open-source conçu pour faciliter la création d'applications alimentées par des modèles de langage (LLMs), comme ChatGPT, Gemini, Claude, Mistral, etc. Il fournit une interface standardisée, des composants réutilisables et des outils puissants pour orchestrer les interactions entre les LLMs, les bases de données, les outils externes et l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3b7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour ! Je vais bien, merci. Et toi, comment vas-tu ?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "call = client.responses.create(  \n",
    "    input = \"Bonjour, comment vas tu\",  # Entrée utilisateur à envoyer au modèle\n",
    "    model = \"gpt-4.1-nano\"  # Nom du modèle à utiliser\n",
    ")\n",
    "\n",
    "print(call.output_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9fb11",
   "metadata": {},
   "source": [
    "### Statelessness ou Absence d’état\n",
    "\n",
    "Le statelessness en intelligence artificielle désigne la capacité d’un modèle à répondre à une requête sans conserver de mémoire des interactions précédentes, chaque requête étant traitée de façon indépendante, sans contexte historique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a6c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour Jack ! Comment puis-je vous aider aujourd'hui ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content( \n",
    "    model=\"gemini-2.0-flash\",  \n",
    "    contents=\"Bonjour je m'appelle jack\", \n",
    ")\n",
    "\n",
    "print(response.text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis un grand modèle linguistique, entraîné par Google. Je n'ai pas de nom.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content( \n",
    "    model=\"gemini-2.0-flash\",  \n",
    "    contents=\"comment je m'apppelle\", \n",
    ")\n",
    "\n",
    "print(response.text)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
